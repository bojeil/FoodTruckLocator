Q: Description of the problem and solution.
A: Create a service that tells the user what types of food trucks might be found near a specific location on a map. The data is available on DataSF: Food Trucks (https://data.sfgov.org/Economy-and-Community/Mobile-Food-Facility-Permit/rqzj-sfat?)

The goal was to make an application that would make the most out of the SF food truck permit data. The data contained included truck location, type of food served, external pdf links containing the truck weekly schedule. The truck schedule being in pdf format required preprocessing. This meant writing a backend script that could parse the entire list of trucks and go over the pdf schedule and parse the different time slots corresponding to each truck item. The pdf formatting posed a challenge especially since the data was displayed in a table format. I found two potential python pdf parsers. Each one had its advantages as well as its disadvantages. The pyPdf module parsed the data into regular text from left to right going top to bottom. This preserved the horizontal order of the table rows. I could get the time of the day for addresses corresponding to the current truck's address. However I couldn't determine the day of the week. This is where I used the pdfminer module. It allowed parsing of the pdf table into an html table. The parsing however didn't convert to an html table, instead it used css div tags with absolute positioning. With some rough estimates I could use the left positioning of the cells containing matching addresses to the current food truck's address to determine their most likely day of week. I could group cells on the same row by checking their top coordinate. Using both modules I was able to merge both data and determine with a good amount of accuracy the day and time of each matching address.

The python script will pick the other important data (company name, street address, geolocation, food type, permit status, truck type, etc) and add the time slots parsed to it and save it in json format.
Instead of parsing each schedule file sequentially, one after the other, I resorted to processing them in parallel using multithreading. I used a model similar to the producer/consumer model where a queue is populated with all the jobs (urls to be parsed) and spawned several fixed number of threads which would each obtain a lock on the queue before popping a task from it and then release it before processing that task and saving the results. After each processing is complete, the same process is repeated until the queue is empty. At the end, the main script would take the shared processed results and output them into a json file.

This provided me with all the backend data that I needed for the front end application. Ideally, I would automatically run the script asynchronously as often as the permit data changes and save the processed data in a database. Instead of saving the data entirely in JSON format, I could create backend scripts with PHP or Python which when queried by the front end application for specific food truck criteria, will filter the data on the server end from the database and build the JSON output and return it to the front end application for display or some additional lightweight filtering.

Instead, with the resources I had and the small amount of data (which was not changing) I was working with, I decided that sticking to the JSON processed file while leaving most of the filtering to the front end app was feasible. 

For the map object, I used Google maps. It allowed me to center the map based on geolocation coordinates and even street addresses. It would also determine geolocation coordinates based on string addresses. This allowed me to center the map at any location the user provides. I also used HTML 5 to determine the user's location (if allowed) to automatically pick the user's location without manual entry. (not very useful for locations outside of data range though). Google map API also allowed me to add markers for every truck on its corresponding location and display info windows on the map with extended truck information. It also allows adding click listeners on marker clicks which I would listen to and on marker click, scroll to the full truck information in an html table summary of all trucks matching current screening criteria.

The front end app food truck form contained the following interface:
Form with following screening elements:
Address: Map center used as a reference for distance screening. This will be refreshed only on manually refresh button click or typing enter.
Distance: Number of distance options (in miled) used as a radius within which results will show. Using the truck items coordinates and the center coordinate, I calculate the distance and if it is out of the specified radius, I exclude it. Changing this field will automatically refresh the results (including the type options).
Type of food: Based on the food types parsed from the truck json file. This is the trivial way of doing it. Ideally the food truck type should be more general and encompassing. It is hard to do that with the data types provided so I stuck with direct string matching. I also added a screen by all types which will display all types. The way I implemented the type screening was by keeping track of all indexes of each truck matching a certain food type. Changing this field will automatically refresh the results without manual refresh click. The total number of results per type is displayed next to each type option.
Availability Date range (day and time): The user provides the day of week and time slot for the start and end. On change, the results are not automatically refreshed. The user will have to click the refresh button. This is since the user has to change the 4 drop down menu in picking a range. There is no point refreshing the results on each pick.
Refresh Button: will refresh truck results and map with the list that meets the form criteria.

Other elements:
Map: Google Map (described earlier), on truck map marker click, the corresponding row containing that truck's info is highlighted and an info window with the truck name and address is displayed.
Results Summary: Sortable table (using jquery plugin sortable on header click) which contains the truck info (name, address and time slots), the type of food served and the distance from the center. On table sort, the sorting criteria is saved so the next time the results are refreshed, the saved sorting criteria is reapplied. On truck company name click, the row is highlighted and the marker on the map corresponding to the truck row will display an info window with the truck name and address. 

Three classes were created, 
The food truck Item class: Food truck object, holds all the truck information and location as well as schedule and all other related functions, takes in the json data object and parses it to initialize object
The map class: map class, takes in the dom object where the map would be loaded, the initial map center coordinates and the function onInit to run when the map is loaded, centers map at any location, looks up addresses and centers map at it, displays overlay info windows at any location, display markers on map and add custom click listeners on marker clicks.
Main controller class: (singleton) the controller builds entire application, loads map, loads truck data, populates form, and displays filtered results in table as well as map, allows all interaction (initializes event listeners to form submit, select field change, results click, map item clicks, etc...)

Q: Whether the solution focuses on back-end, front-end or if it's full stack.
A: The solution I implemented focuses on front end. One reason is due to the limited resources I had to begin with. On the other the solution was able to implement all expected functionality for the given pool of data and it did so in an efficient manner. The only thing I couldn't implement with a front end script was the scheduling for each truck. The schedules for each truck are presented in PDF format hosted online. In order to implement screening based on schedule, I had to parse the main data file, retrieve the schedules for each truck, load them, process them and then save them in a format which can be used by the front end application. This could have been done in various ways. I decided to write a python script which I ran once to parse and process all the data for each truck and saved them in a modified json file (on the hosting server along with the front end source codes) which would be used as the main data for the front end application.

Q: Reasoning behind your technical choices, including architectural. Trade-offs you might have made, anything you left out, or what you might do differently if you were to spend additional time on the project.
A: 
The goal of the application was to screen for food trucks in a specified region (by proximity) with the ability of screening by food types and time of day. Except for the screening by day of week/time of day, all the functionality could be achieved by pure front end processing. However since the truck schedule was provided separately in a pdf file per truck, the only way to achieve that was with backend preprocessing. Since the data does not seem to change, I could write a backend script which runs asynchronously to parse all the files, process them and output them in json format to be used by the front end application for screening. Since the data is limited in size, this was feasible and didn't require backend synchronous processing per screen request or the need for database storage. So I ended up running the python script I wrote once and saved the resulting json file along with the front end source code.
As for technical choices, I chose Google map api since it provided me with all the map functionality necessary for this application including determining geocoding information of text addresses. I chose jQuery as the javascript framework for its ease in searching and manipulating html dom objects independent of browser type.
As for the backend script, I chose python mainly due to the python thrid party pdf parsing modules, json editing capabilities available for it, its wide support and since it is quite handy for writing quick scripts as well as its multithreading capabilities. 
As for the data storage, I chose JSON since it has wide support especially on the front end side via javascript. In addition the original data source had that format available and Python had the ability to parse it as well as convert dictionary data to json format.

The main structure I followed would apply even if I set up backend dynamic scripting and a database. In that case I would run my python script once a day to update the data and save it in a database instead of directly in a json file. This way I could scale the application to handle a lot more data and move the bulk of the processing and filtering from the front end to the back end, and basically generate intermediate json files on front end request (on demand) populated by the processed data in the database.
I could invest more in a pdf parser for parsing pdf tables into CSV format or html tables that use table tags since they are easier and more accurate to parse. I could also improve the front end interface, especially for mobile devices. Currently the application will work on mobile browsers but for large results, the table grows too long and the map is pushed to the bottom of the page. In addition I could save user preferences using cookies for example so that the next time the page is loaded, the preferences are repopulated. 
One major improvement would be the screen by type of food. I could look for food keywords. This is harder to implement but important for such an application. The problem is each provided food truck description is not specific to a certain general criteria. Some smart type matching will need to be designed for this and run during the backend parsing process and then saved to the output json file or database table.
The backend parsing script can be optimized further for scheduling accuracy and most of all for speed of execution.

Q: Link to other code you're particularly proud of.
A: http://stage.bollingeronbollingerbands.com:8082/chartx/chart.html
	http://www.bollingerbands.com/shared/libraries/slideshow/sample2.php
	http://www.bollingerbands.com/shared/libraries/slideshow/sample.php

Q: Link to your resume or public profile
A: https://www.linkedin.com/in/bassamojeil

Q: Link to application code.
A: https://github.com/bojeil/FoodTruckLocator/tree/gh-pages

Q: Link to to the hosted application where applicable.
A: http://bojeil.github.io/FoodTruckLocator/